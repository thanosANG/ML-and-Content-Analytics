{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ba625f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PREPARE DATASET\n",

    "\n",
    "#import logging\n",
    "#logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#SELECTED APPROPRIATE DATA\n",
    "\n",
    "selected_cols =['product_id','star_rating','helpful_votes','total_votes','review_headline','review_body']\n",
    "# Read TSV file into DataFrame\n",
    "df = pd.read_csv('amazon_reviews_us_Mobile_Electronics_v1_00.tsv', sep='\\t', on_bad_lines='skip',usecols=selected_cols) \n",
    "df = df.loc[(df.helpful_votes > df.total_votes / 2) & (df.helpful_votes > 1)]\n",
    "df=df.reset_index(drop=True)\n",
    "\n",
    "#Select a spesific product\n",
    "product = df.loc[df.product_id==\"B002MWYUFU\"]\n",
    "product[\"review_body\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb410ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "\n",
    "#We initialize a document store in order to create embedings of the reviews\n",
    "#We do this, to perform semantic search (Retrieval step)\n",
    "#If computation time is an issue, we can use BM25 \n",
    "\n",
    "from haystack.document_stores import PineconeDocumentStore\n",
    "from haystack.nodes import PreProcessor\n",
    "\n",
    "document_store = PineconeDocumentStore(\n",
    "    api_key='8c39bd74-3fa9-436a-b775-f0ddb9073e9c', \n",
    "    index='generative-review',\n",
    "    similarity=\"cosine\",\n",
    "    embedding_dim=768,environment='asia-southeast1-gcp-free'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2331fe6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import Document\n",
    "from haystack.nodes import PreProcessor\n",
    "from tqdm.auto import tqdm  # progress bar\n",
    "\n",
    "#This cell is used to pass some data in Pinecone to experiment\n",
    "#A preprocessor is also initialized, to split reviews in appropriate lengths.\n",
    "#The deep learning model can process a limited number of tokens per review.\n",
    "\n",
    "total_doc_count = product.shape[0]\n",
    "batch_size = 50\n",
    "\n",
    "#initialize preprocessor\n",
    "preprocessor = PreProcessor(\n",
    "    clean_whitespace=True,\n",
    "    clean_header_footer=True,\n",
    "    clean_empty_lines=True,\n",
    "    split_by=\"word\",\n",
    "    split_length=170,\n",
    "    split_overlap=15,\n",
    "    split_respect_sentence_boundary=True,\n",
    ")\n",
    "\n",
    "\n",
    "counter = 0\n",
    "docs = []\n",
    "for d,r in tqdm(product.iterrows(), total=total_doc_count):\n",
    "    \n",
    "    # create haystack document object with text content and doc metadata\n",
    "    doc = Document(\n",
    "        content=r[\"review_body\"],\n",
    "        meta={\n",
    "            \"product_id\": r[\"product_id\"],\n",
    "        }\n",
    "    )\n",
    "    docs.append(doc)\n",
    "    counter+=1\n",
    "    if counter == batch_size:\n",
    "        split_documents = preprocessor.process(docs)\n",
    "        document_store.write_documents(split_documents)\n",
    "        docs.clear()\n",
    "    if counter == total_doc_count:\n",
    "        split_documents = preprocessor.process(docs)\n",
    "        document_store.write_documents(docs)\n",
    "        break\n",
    "\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8adecd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#CREATE EMBEDINGS\n",
    "from haystack.nodes import EmbeddingRetriever\n",
    "\n",
    " \n",
    "\n",
    "retriever = EmbeddingRetriever(\n",
    "   document_store=document_store,\n",
    "   embedding_model=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "   model_format=\"sentence_transformers\",top_k=30\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63aa9221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #TEST RETRIVER\n",
    "\n",
    "# from haystack.pipelines import DocumentSearchPipeline\n",
    "# from haystack.utils import print_documents\n",
    "\n",
    "# search_pipe = DocumentSearchPipeline(retriever)\n",
    "# result = search_pipe.run(\n",
    "#     query=\"What are some negative comments about the characteristics?\",\n",
    "#     params={\"Retriever\": {\"top_k\": 5}}\n",
    "# )\n",
    "\n",
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cb2e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.nodes import PromptNode, PromptTemplate, AnswerParser\n",
    "#\"\"\"Synthesize a comprehensive answer from the following  most relevant paragraphs and the given question. Provide a clear and concise response that summarizes the key points and information presented in the paragraphs.\n",
    "\n",
    "#Initialize prompt to be passed to GPT 3.5, to get an answer\n",
    "\n",
    "openai_api_key=\"key\"\n",
    "rag_prompt = PromptTemplate(\n",
    "    prompt=\"\"\"Given a question related to a product, please use the information from a collection of product reviews to generate a concise and informative answer. \n",
    "              The system should consider multiple perspectives and opinions expressed in the reviews to provide a balanced response. Ensure that the answer is coherent, relevant, and based on the sentiments and insights gathered from the reviews. \n",
    "                              \n",
    "                             \\n\\n Paragraphs: {join(documents, pattern='$summary')} \\n\\n Question: {query} \\n\\n Answer:\"\"\",\n",
    "    output_parser=AnswerParser(),\n",
    ")\n",
    "prompt_node=PromptNode(model_name_or_path=\"gpt-3.5-turbo\", api_key=openai_api_key,default_prompt_template=rag_prompt,max_length=200)\n",
    "\n",
    "\n",
    "#{join(documents)}\n",
    "#{join(documents, pattern='$summary')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb41e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.nodes import TransformersSummarizer\n",
    "\n",
    "summarizer = TransformersSummarizer(model_name_or_path=\"ThanosAng/Product_Review_Summary\",max_length= 100,min_length=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6294e651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will be called if the language is not english. The idea is to support questions in languages different than english\n",
    "# Through this process, the user can simply write in his own language without performing any other action and the appropriate\n",
    "# models will be automatically be chosen to create an answer and translate back. This is done through a combination of classification mode\\\n",
    "# that classifies the language in the question, and translation models.\n",
    "\n",
    "#parameters: question= the original question from the user\n",
    "#            to_eng= model that translates question to egnlish\n",
    "#            to_foreign = model that translates the answer back to original language \n",
    "\n",
    "#It can be any language (supported by the classification model), but we test just for french\n",
    "#Search \"papluca/xlm-roberta-base-language-detection\" for more information\n",
    "\n",
    "def foreign_input(question,to_eng,to_foreign)\n",
    "\n",
    "    translator_input  = TransformersTranslator(model_name_or_path=to_eng)\n",
    "    translator_output = TransformersTranslator(model_name_or_path=to_foreign)\n",
    "\n",
    "    node = PromptNode()\n",
    "    pipeline = Pipeline()\n",
    "    \n",
    "    # Translator for input query\n",
    "    pipeline.add_node(component=translator_input, name=\"Translator_Input\", inputs=[\"Query\"])\n",
    "\n",
    "    # Retriever\n",
    "    pipeline.add_node(component=retriever, name='Retriever', inputs=['Translator_Input'])\n",
    "\n",
    "    # Summarizer\n",
    "    pipeline.add_node(component=summarizer, name=\"Summarizer\", inputs=[\"Retriever\"])\n",
    "\n",
    "    pipeline.add_node(component=prompt_node, name=\"prompt_node\", inputs=[\"Summarizer\"])\n",
    "\n",
    "    # Run the pipeline\n",
    "    res = pipeline.run(query=\"Y a-t-il des reflets?\")\n",
    "\n",
    "    #Finally, translate back to French\n",
    "\n",
    "    DOCS = [\n",
    "            Document(\n",
    "                content=res[\"answers\"][0].answer\n",
    "            )\n",
    "        ]\n",
    "    \n",
    "    res = translator_output.translate(documents=DOCS, query=None)\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de14272f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function is called when question was classified as english.\n",
    "def QA_english(question)    \n",
    "\n",
    "    node = PromptNode()\n",
    "    pipeline = Pipeline()\n",
    "\n",
    "    # Retriever\n",
    "    pipeline.add_node(component=retriever, name='Retriever', inputs=['Query'])\n",
    "\n",
    "    # Summarizer\n",
    "    pipeline.add_node(component=summarizer, name=\"Summarizer\", inputs=[\"Retriever\"])\n",
    "\n",
    "    pipeline.add_node(component=prompt_node, name=\"prompt_node\", inputs=[\"Summarizer\"])\n",
    "\n",
    "    # Run the pipeline\n",
    "    res = pipeline.run(query=question)\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f70f38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "#Initialize the classification model\n",
    "pipe = pipeline(\"text-classification\", model=\"papluca/xlm-roberta-base-language-detection\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2f2082",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import Document\n",
    "from haystack.pipelines import Pipeline\n",
    "from haystack.nodes import TransformersTranslator\n",
    "\n",
    "# We initialize a dictionary where the keys are the outputs of the classification model (different languages)\\\n",
    "# and as values, the models  to be used. For demonstration purposes we only added english, german and french, but we could insert\\\n",
    "# any other language (as long as its supported by the classification model).\n",
    "\n",
    "#The classification model will run, classify the language of question, and choose the appropriate model, which will be loaded\\\n",
    "#automatically in the pipeline.\n",
    "\n",
    "# Available languages are: arabic (ar), bulgarian (bg), german (de), modern greek (el), english (en), spanish (es), french (fr), hindi (hi), \n",
    "#                          italian (it), japanese (ja), dutch (nl), polish (pl), portuguese (pt), russian (ru), swahili (sw), thai (th), turkish (tr), \n",
    "#                          urdu (ur), vietnamese (vi), and chinese (zh)\n",
    "\n",
    "\n",
    "lang_to_eng = {\"fr\":\"Helsinki-NLP/opus-mt-fr-en\",\"de\",\"Helsinki-NLP/opus-mt-de-en\",\"en\":\"en\"} #The dict contains models that map the language to english\n",
    "lang_to_foreign={\"fr:Helsinki-NLP/opus-mt-en-fr\",\"de\",\"Helsinki-NLP/opus-mt-en-de\"}    #The dict contains models that map english back to foreign\n",
    "\n",
    "#Set a question\n",
    "question = \"L'écran reflète-t-il\"\n",
    "\n",
    "#Classify the language (identify it)\n",
    "language=pipe(question)[0][\"label\"]\n",
    "\n",
    "#Call the functions\n",
    "if language ==\"en\":\n",
    "    QA_english(question)   \n",
    "else:\n",
    "    model_to_foreign= lang_to_eng[language] #Define the model to be used to trnaslate to english\n",
    "    model_to_english= lang_to_foreign[language] #Define the model that will translate back to foreign\n",
    "    QA_foreign(question,model_to_foreign,model_to_english)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a42e027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #DELETE CONTENTS OF INDEX\n",
    "\n",
    "# pinecone.init(api_key='8c39bd74-3fa9-436a-b775-f0ddb9073e9c', environment='asia-southeast1-gcp-free') \n",
    "# index = pinecone.Index('generative-review') \n",
    "\n",
    "# delete_response = index.delete(deleteAll=True,namespace=\"no-vectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74acf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# # confirm GPU is available, outputs True if so\n",
    "# torch.cuda.is_available()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch_gpu)",
   "language": "python",
   "name": "torch_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
